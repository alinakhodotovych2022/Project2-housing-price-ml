{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alinakhodotovych2022/Project2-housing-price-ml/blob/main/Project2_housing_price_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d1238e0",
      "metadata": {
        "id": "8d1238e0"
      },
      "source": [
        "# Housing Price Prediction Using Zillow ZHVI (ZIP-Level)\n",
        "\n",
        "_Data Science Project — Machine Learning Modeling & Pipeline (Project 2)_\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"./image Project 2.jpeg\" alt=\"Housing Price Banner\" width=\"700\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3cbdd2",
      "metadata": {
        "id": "9d3cbdd2"
      },
      "source": [
        "## 1. Project Overview (BLUF)\n",
        "\n",
        "**Bottom Line Up Front:**  \n",
        "This project builds a **reproducible machine learning regression pipeline** that predicts home values (Zillow Home Value Index, ZHVI) at the **ZIP code level** using historical ZHVI time series data from **Zillow Research**.\n",
        "\n",
        "The pipeline includes:\n",
        "\n",
        "- Data loading and cleaning of the wide monthly ZHVI dataset  \n",
        "- Reshaping from wide format (many date columns) to modeling-ready features  \n",
        "- Exploratory Data Analysis (EDA) with clear visualizations  \n",
        "- Feature engineering on recent monthly ZHVI history  \n",
        "- Baseline and advanced regression models (e.g., Linear Regression, Ridge, Random Forest)  \n",
        "- **Scikit-learn `Pipeline`** with preprocessing, feature engineering, and model in one object  \n",
        "- **Hyperparameter tuning** with cross-validation  \n",
        "- Final model evaluation using MAE, RMSE, and R²  \n",
        "- **Serialization of the trained pipeline** for reuse (`housing_price_pipeline.pkl`)\n",
        "\n",
        "Business question in simple terms:\n",
        "\n",
        "> _“Given recent price dynamics for each ZIP code, what is a reasonable prediction of the current home value (ZHVI)?”_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34f7917",
      "metadata": {
        "id": "f34f7917"
      },
      "source": [
        "## 2. Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d767c96",
      "metadata": {
        "id": "3d767c96"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Import core libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Configure plots to be a bit larger by default\n",
        "plt.rcParams['figure.figsize'] = (8, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e99d542c",
      "metadata": {
        "id": "e99d542c"
      },
      "source": [
        "### 2.1 Load Zillow ZHVI ZIP-level dataset\n",
        "\n",
        "In Colab we can upload the CSV directly.  \n",
        "If you run this locally, you can skip the `files.upload()` part and read the CSV from a file path instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b6ee3f",
      "metadata": {
        "id": "e8b6ee3f"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Load the ZHVI ZIP-level CSV file\n",
        "# Option A (Google Colab upload):\n",
        "# --------------------------------\n",
        "try:\n",
        "    from google.colab import files  # Will fail if not in Colab\n",
        "    uploaded = files.upload()\n",
        "    csv_filename = list(uploaded.keys())[0]\n",
        "except Exception:\n",
        "    csv_filename = \"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"  # adjust if needed\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(csv_filename)\n",
        "\n",
        "# Quick checks\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d51bca8f",
      "metadata": {
        "id": "d51bca8f"
      },
      "source": [
        "### 2.2 Initial Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d10abe",
      "metadata": {
        "id": "33d10abe"
      },
      "outputs": [],
      "source": [
        "# STEP 2: Basic dataset structure and summary\n",
        "\n",
        "print(\"Shape (rows, columns):\", df.shape)\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc62dca6",
      "metadata": {
        "id": "bc62dca6"
      },
      "source": [
        "## 3. Data Preparation & Reshaping\n",
        "\n",
        "The Zillow ZIP-level file is in **wide format**: each row is a ZIP code and each date (YYYY-MM-DD) is a separate column.  \n",
        "To build features, we first need to:\n",
        "\n",
        "1. Identify which columns are **date columns**.  \n",
        "2. Sort those date columns in chronological order.  \n",
        "3. Decide which **latest month** we want to predict (our target).  \n",
        "4. Construct features from the recent history (e.g., last 12 months)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c88d408",
      "metadata": {
        "id": "2c88d408"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Identify date columns (monthly ZHVI values)\n",
        "\n",
        "# Non-date columns that describe the region\n",
        "meta_cols = [\n",
        "    \"RegionID\", \"SizeRank\", \"RegionName\", \"RegionType\",\n",
        "    \"StateName\", \"State\", \"City\", \"Metro\", \"CountyName\"\n",
        "]\n",
        "\n",
        "# Treat all other columns as candidate date columns\n",
        "candidate_date_cols = [c for c in df.columns if c not in meta_cols]\n",
        "\n",
        "# Many of these columns are date-like strings such as '2000-01-31'\n",
        "# We keep only those that can be safely parsed as dates\n",
        "date_cols = []\n",
        "for c in candidate_date_cols:\n",
        "    try:\n",
        "        pd.to_datetime(c)\n",
        "        date_cols.append(c)\n",
        "    except Exception:\n",
        "        # skip non-date columns if any\n",
        "        pass\n",
        "\n",
        "# Sort the date columns chronologically\n",
        "date_cols = sorted(date_cols, key=lambda x: pd.to_datetime(x))\n",
        "\n",
        "print(f\"Number of monthly ZHVI columns: {len(date_cols)}\")\n",
        "print(\"First 5 date columns:\", date_cols[:5])\n",
        "print(\"Last 5 date columns:\", date_cols[-5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea65b01f",
      "metadata": {
        "id": "ea65b01f"
      },
      "source": [
        "### 3.1 Define target month and feature window\n",
        "\n",
        "We will:\n",
        "\n",
        "- Use the **latest available month** as the regression target `y`.  \n",
        "- Use the **12 months immediately before the target** as raw numeric features.  \n",
        "- Later, we will also derive **growth-rate features** from this 12‑month history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8a631d",
      "metadata": {
        "id": "0d8a631d"
      },
      "outputs": [],
      "source": [
        "# STEP 4: Define target month and feature window\n",
        "\n",
        "# Use the last available month as the prediction target\n",
        "target_col = date_cols[-1]\n",
        "print(\"Target month column:\", target_col)\n",
        "\n",
        "# Choose a 12‑month window before the target as base numeric features\n",
        "window_months = 12\n",
        "feature_date_cols = date_cols[-(window_months + 1):-1]  # previous 12 months\n",
        "print(\"\\nFeature window columns (12 months before target):\")\n",
        "print(feature_date_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a57b45e",
      "metadata": {
        "id": "9a57b45e"
      },
      "source": [
        "### 3.2 Build base modeling dataset\n",
        "\n",
        "We keep only rows where both the target month and the 12-month window are non-null.  \n",
        "This ensures the model trains on complete histories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708ac5d2",
      "metadata": {
        "id": "708ac5d2"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Filter rows with complete data for the chosen window + target\n",
        "\n",
        "cols_needed = meta_cols + feature_date_cols + [target_col]\n",
        "model_df = df[cols_needed].dropna()\n",
        "\n",
        "print(\"Shape after dropping missing rows:\", model_df.shape)\n",
        "\n",
        "# Rename target for convenience\n",
        "model_df = model_df.rename(columns={target_col: \"target_zhvi\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef81437",
      "metadata": {
        "id": "0ef81437"
      },
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "From the 12‑month ZHVI history we generate additional features, for example:\n",
        "\n",
        "- Average ZHVI over the last 12 months  \n",
        "- Standard deviation (volatility)  \n",
        "- 12‑month growth rate  \n",
        "- 3‑month and 6‑month growth rates  \n",
        "- Relative position of the latest month vs. 12‑month mean  \n",
        "\n",
        "We also keep region descriptors such as **State** and **City** as categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c645d77",
      "metadata": {
        "id": "8c645d77"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Create numeric time‑series features from the 12‑month window\n",
        "\n",
        "feature_df = model_df.copy()\n",
        "\n",
        "# Convert the feature window into a numpy array for calculations\n",
        "window_values = feature_df[feature_date_cols].values\n",
        "\n",
        "# Basic statistics\n",
        "feature_df[\"zhvi_mean_12m\"] = window_values.mean(axis=1)\n",
        "feature_df[\"zhvi_std_12m\"] = window_values.std(axis=1)\n",
        "\n",
        "# Last month in the window (just before target)\n",
        "last_window_col = feature_date_cols[-1]\n",
        "feature_df[\"zhvi_last_before_target\"] = feature_df[last_window_col]\n",
        "\n",
        "# Growth features\n",
        "first_window_col = feature_date_cols[0]\n",
        "feature_df[\"growth_12m_abs\"] = feature_df[last_window_col] - feature_df[first_window_col]\n",
        "feature_df[\"growth_12m_pct\"] = feature_df[\"growth_12m_abs\"] / feature_df[first_window_col]\n",
        "\n",
        "# 6‑month growth (if at least 6 months in window)\n",
        "if len(feature_date_cols) >= 6:\n",
        "    col_6m_ago = feature_date_cols[-6]\n",
        "    feature_df[\"growth_6m_abs\"] = feature_df[last_window_col] - feature_df[col_6m_ago]\n",
        "    feature_df[\"growth_6m_pct\"] = feature_df[\"growth_6m_abs\"] / feature_df[col_6m_ago]\n",
        "\n",
        "# 3‑month growth (if at least 3 months in window)\n",
        "if len(feature_date_cols) >= 3:\n",
        "    col_3m_ago = feature_date_cols[-3]\n",
        "    feature_df[\"growth_3m_abs\"] = feature_df[last_window_col] - feature_df[col_3m_ago]\n",
        "    feature_df[\"growth_3m_pct\"] = feature_df[\"growth_3m_abs\"] / feature_df[col_3m_ago]\n",
        "\n",
        "# Relative position vs. mean\n",
        "feature_df[\"last_vs_mean_ratio\"] = feature_df[last_window_col] / feature_df[\"zhvi_mean_12m\"]\n",
        "\n",
        "print(\"Feature dataframe shape (with engineered features):\", feature_df.shape)\n",
        "feature_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d411a9",
      "metadata": {
        "id": "84d411a9"
      },
      "source": [
        "### 4.1 Final feature set\n",
        "\n",
        "We select:\n",
        "\n",
        "- **Numeric features**: engineered statistics and growth rates  \n",
        "- **Categorical features**: `State` and `Metro`  \n",
        "- **Target**: `target_zhvi`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0125a4",
      "metadata": {
        "id": "3b0125a4"
      },
      "outputs": [],
      "source": [
        "# STEP 7: Define feature matrix X and target vector y\n",
        "\n",
        "numeric_features = [\n",
        "    \"zhvi_mean_12m\", \"zhvi_std_12m\",\n",
        "    \"zhvi_last_before_target\",\n",
        "    \"growth_12m_abs\", \"growth_12m_pct\",\n",
        "    \"growth_6m_abs\", \"growth_6m_pct\",\n",
        "    \"growth_3m_abs\", \"growth_3m_pct\",\n",
        "    \"last_vs_mean_ratio\"\n",
        "]\n",
        "\n",
        "categorical_features = [\"State\", \"Metro\"]\n",
        "\n",
        "# X = selected features; y = target\n",
        "X = feature_df[numeric_features + categorical_features]\n",
        "y = feature_df[\"target_zhvi\"]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32d9fd9f",
      "metadata": {
        "id": "32d9fd9f"
      },
      "source": [
        "## 5. Exploratory Data Analysis (EDA)\n",
        "\n",
        "We now explore the data to understand:\n",
        "\n",
        "- Distribution of the target (current ZHVI)  \n",
        "- Relationship between engineered features and target  \n",
        "- State-level patterns  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a787ab6",
      "metadata": {
        "id": "0a787ab6"
      },
      "outputs": [],
      "source": [
        "# STEP 8: Target distribution\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(y, bins=40)\n",
        "plt.title(\"Distribution of Target ZHVI (Latest Month)\")\n",
        "plt.xlabel(\"ZHVI\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0a679b",
      "metadata": {
        "id": "4e0a679b"
      },
      "outputs": [],
      "source": [
        "# STEP 9: Example relationship — 12‑month growth vs target\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(feature_df[\"growth_12m_pct\"], y, alpha=0.3)\n",
        "plt.title(\"12‑Month Growth Rate vs Target ZHVI\")\n",
        "plt.xlabel(\"12‑Month Growth Rate (pct)\")\n",
        "plt.ylabel(\"Target ZHVI\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a4ba80",
      "metadata": {
        "id": "c0a4ba80"
      },
      "outputs": [],
      "source": [
        "# STEP 10: Average target ZHVI by State (top 10 states by mean)\n",
        "\n",
        "state_means = feature_df.groupby(\"State\")[\"target_zhvi\"].mean().sort_values(ascending=False).head(10)\n",
        "print(state_means)\n",
        "\n",
        "plt.figure()\n",
        "state_means.plot(kind=\"bar\")\n",
        "plt.title(\"Top 10 States by Average Target ZHVI\")\n",
        "plt.ylabel(\"Average ZHVI\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee79c8f7",
      "metadata": {
        "id": "ee79c8f7"
      },
      "source": [
        "## 6. Train/Test Split\n",
        "\n",
        "We split the data into **training** and **test** sets using a random split, since each row corresponds to a different ZIP code (not a time-series split per ZIP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6ff706",
      "metadata": {
        "id": "ef6ff706"
      },
      "outputs": [],
      "source": [
        "# STEP 11: Train/test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1635389b",
      "metadata": {
        "id": "1635389b"
      },
      "source": [
        "## 7. Baseline Model and Preprocessing Pipeline\n",
        "\n",
        "Before advanced models, we create:\n",
        "\n",
        "1. A **baseline model** using `DummyRegressor` (predicts median).  \n",
        "2. A **preprocessing pipeline** with:\n",
        "   - `StandardScaler` for numeric features  \n",
        "   - `OneHotEncoder` for categorical features  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77fca9b",
      "metadata": {
        "id": "d77fca9b"
      },
      "outputs": [],
      "source": [
        "# STEP 12: Baseline model (DummyRegressor)\n",
        "\n",
        "baseline = DummyRegressor(strategy=\"median\")\n",
        "baseline.fit(X_train, y_train)\n",
        "baseline_preds = baseline.predict(X_test)\n",
        "\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_preds)\n",
        "baseline_rmse = mean_squared_error(y_test, baseline_preds, squared=False)\n",
        "baseline_r2 = r2_score(y_test, baseline_preds)\n",
        "\n",
        "print(\"Baseline MAE:\", baseline_mae)\n",
        "print(\"Baseline RMSE:\", baseline_rmse)\n",
        "print(\"Baseline R^2:\", baseline_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6177966c",
      "metadata": {
        "id": "6177966c"
      },
      "outputs": [],
      "source": [
        "# STEP 13: Build preprocessing transformer for numeric + categorical features\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1301647",
      "metadata": {
        "id": "e1301647"
      },
      "source": [
        "## 8. Modeling Strategy\n",
        "\n",
        "We evaluate several regression algorithms using the same preprocessing:\n",
        "\n",
        "- **Linear Regression** (baseline ML model)  \n",
        "- **Ridge Regression** (L2-regularized linear model)  \n",
        "- **Random Forest Regressor** (non-linear tree ensemble)\n",
        "\n",
        "We use cross-validation on the training set to compare models, then perform hyperparameter tuning on the best ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0648429c",
      "metadata": {
        "id": "0648429c"
      },
      "outputs": [],
      "source": [
        "# STEP 14: Helper function for evaluation\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name=\"model\"):\n",
        "    \"\"\"Fit model, compute CV score, and evaluate on test set.\"\"\"\n",
        "    # Cross-validation on training set\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\")\n",
        "    cv_rmse = np.mean(np.sqrt(-cv_scores))\n",
        "\n",
        "    # Fit on full training data\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    print(\"CV RMSE (mean over folds):\", cv_rmse)\n",
        "    print(\"Test MAE:\", mae)\n",
        "    print(\"Test RMSE:\", rmse)\n",
        "    print(\"Test R^2:\", r2)\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"cv_rmse\": cv_rmse,\n",
        "        \"test_mae\": mae,\n",
        "        \"test_rmse\": rmse,\n",
        "        \"test_r2\": r2,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2091dbb9",
      "metadata": {
        "id": "2091dbb9"
      },
      "outputs": [],
      "source": [
        "# STEP 15: Define models wrapped in Pipelines with shared preprocessing\n",
        "\n",
        "linreg_model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "ridge_model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "\n",
        "rf_model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", RandomForestRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_model(linreg_model, X_train, y_train, X_test, y_test, \"Linear Regression\"))\n",
        "results.append(evaluate_model(ridge_model, X_train, y_train, X_test, y_test, \"Ridge Regression\"))\n",
        "results.append(evaluate_model(rf_model, X_train, y_train, X_test, y_test, \"Random Forest\"))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1d765a",
      "metadata": {
        "id": "8b1d765a"
      },
      "source": [
        "## 9. Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "Based on initial results, we tune **Ridge Regression** using a grid of `alpha` values.  \n",
        "This step demonstrates how to integrate preprocessing + model selection into a single `GridSearchCV` pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987e3ce1",
      "metadata": {
        "id": "987e3ce1"
      },
      "outputs": [],
      "source": [
        "# STEP 16: Hyperparameter tuning for Ridge Regression\n",
        "\n",
        "ridge_pipeline = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", Ridge())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"model__alpha\": [0.1, 1, 10, 50, 100]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    ridge_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Best CV score (negative MSE):\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57313667",
      "metadata": {
        "id": "57313667"
      },
      "outputs": [],
      "source": [
        "# STEP 17: Final evaluation of tuned Ridge model\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "final_preds = best_model.predict(X_test)\n",
        "\n",
        "final_mae = mean_absolute_error(y_test, final_preds)\n",
        "final_rmse = mean_squared_error(y_test, final_preds, squared=False)\n",
        "final_r2 = r2_score(y_test, final_preds)\n",
        "\n",
        "print(\"Final tuned Ridge MAE:\", final_mae)\n",
        "print(\"Final tuned Ridge RMSE:\", final_rmse)\n",
        "print(\"Final tuned Ridge R^2:\", final_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88fdc47",
      "metadata": {
        "id": "b88fdc47"
      },
      "source": [
        "## 10. Model Diagnostics & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9671ad82",
      "metadata": {
        "id": "9671ad82"
      },
      "outputs": [],
      "source": [
        "# STEP 18: Actual vs Predicted scatter plot\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(y_test, final_preds, alpha=0.4)\n",
        "plt.title(\"Actual vs Predicted ZHVI (Tuned Ridge Model)\")\n",
        "plt.xlabel(\"Actual ZHVI\")\n",
        "plt.ylabel(\"Predicted ZHVI\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\")  # diagonal line\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb580dc",
      "metadata": {
        "id": "7eb580dc"
      },
      "outputs": [],
      "source": [
        "# STEP 19: Residuals histogram\n",
        "\n",
        "residuals = y_test - final_preds\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(residuals, bins=40)\n",
        "plt.title(\"Residual Distribution (Actual - Predicted)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e739291c",
      "metadata": {
        "id": "e739291c"
      },
      "outputs": [],
      "source": [
        "# STEP 20: Approximate feature importance for numeric features (Ridge coefficients)\n",
        "\n",
        "# Extract the trained Ridge model from the pipeline\n",
        "ridge_step = best_model.named_steps[\"model\"]\n",
        "\n",
        "# Get one‑hot encoder output feature names for categorical variables\n",
        "ohe = best_model.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n",
        "cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
        "\n",
        "all_feature_names = numeric_features + list(cat_feature_names)\n",
        "\n",
        "# Coefficients aligned with transformed feature space\n",
        "coeffs = ridge_step.coef_\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    \"feature\": all_feature_names,\n",
        "    \"coefficient\": coeffs\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value and show top 15\n",
        "importance_df[\"abs_coef\"] = importance_df[\"coefficient\"].abs()\n",
        "top_importance = importance_df.sort_values(\"abs_coef\", ascending=False).head(15)\n",
        "\n",
        "display(top_importance)\n",
        "\n",
        "plt.figure()\n",
        "plt.barh(top_importance[\"feature\"], top_importance[\"coefficient\"])\n",
        "plt.title(\"Top 15 Features by Ridge Coefficient\")\n",
        "plt.xlabel(\"Coefficient\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15b4463",
      "metadata": {
        "id": "f15b4463"
      },
      "source": [
        "## 11. Save Trained Pipeline\n",
        "\n",
        "We serialize the final tuned pipeline using `joblib`.  \n",
        "This file can be loaded later to generate predictions on new data without retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a299e9f",
      "metadata": {
        "id": "7a299e9f"
      },
      "outputs": [],
      "source": [
        "# STEP 21: Save the trained pipeline to disk\n",
        "\n",
        "pipeline_filename = \"housing_price_pipeline.pkl\"\n",
        "joblib.dump(best_model, pipeline_filename)\n",
        "\n",
        "print(f\"Saved trained pipeline to: {pipeline_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a17039",
      "metadata": {
        "id": "99a17039"
      },
      "source": [
        "## 12. Business Interpretation & Next Steps\n",
        "\n",
        "### 12.1 Interpretation of Results\n",
        "\n",
        "- The tuned Ridge model achieves **lower error metrics than the baseline** median regressor, indicating that recent 12‑month ZHVI history contains useful predictive signal.  \n",
        "- Growth-related features (12‑month and 6‑month percentage changes) and recent level (`zhvi_last_before_target`) typically have the strongest coefficients, which aligns with intuition about housing markets.  \n",
        "- State- and metro-level one‑hot encoded features capture regional pricing differences that are not explained by recent dynamics alone.\n",
        "\n",
        "### 12.2 Example Use Cases\n",
        "\n",
        "Stakeholders who might benefit from this model:\n",
        "\n",
        "- **Real estate investors**: to compare ZIP codes and identify areas with strong momentum but still moderate prices.  \n",
        "- **Lenders / banks**: to stress‑test portfolios under different regional value assumptions.  \n",
        "- **Policy analysts or city planners**: to understand how local markets differ in level and growth.\n",
        "\n",
        "### 12.3 Limitations\n",
        "\n",
        "- The model uses only **historical ZHVI data** — it does not include macroeconomic variables (rates, income, employment, etc.).  \n",
        "- We treat each ZIP as independent and perform a random train/test split, which does not fully respect temporal dynamics at the national level.  \n",
        "- Out‑of‑sample performance may degrade if market conditions change dramatically relative to the training period.\n",
        "\n",
        "### 12.4 Possible Extensions\n",
        "\n",
        "- Add **macro features** (unemployment rate, mortgage rates, income, construction permits).  \n",
        "- Use **time‑series models** (e.g., gradient boosting with lag features or dedicated sequence models).  \n",
        "- Tune and compare additional algorithms (e.g., Gradient Boosting, XGBoost, LightGBM) and perform deeper hyperparameter searches.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}